{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import subprocess, sys\n",
    "\n",
    "# def install(pkg):\n",
    "#     try:\n",
    "#         __import__(pkg.replace(\"-\",\"_\").replace(\"scikit_learn\",\"sklearn\"))\n",
    "#     except ImportError:\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
    "\n",
    "# for p in [\"pandas\",\"numpy\",\"matplotlib\",\"seaborn\",\"scikit-learn\",\"scipy\"]:\n",
    "#     install(p)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib as plt\n",
    "# matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded successfully!\n",
      " Shape: 440 rows × 8 columns\n",
      "\n",
      "Columns: ['Channel', 'Region', 'Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
      "\n",
      "First 5 rows:\n",
      "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
      "0        2       3  12669  9656     7561     214              2674        1338\n",
      "1        2       3   7057  9810     9568    1762              3293        1776\n",
      "2        2       3   6353  8808     7684    2405              3516        7844\n",
      "3        1       3  13265  1196     4221    6404               507        1788\n",
      "4        2       3  22615  5410     7198    3915              1777        5185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 1: DOWNLOAD REAL-WORLD DATA\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "print(f\" Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head().to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de834693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Types ---\n",
      "Channel             int64\n",
      "Region              int64\n",
      "Fresh               int64\n",
      "Milk                int64\n",
      "Grocery             int64\n",
      "Frozen              int64\n",
      "Detergents_Paper    int64\n",
      "Delicassen          int64\n",
      "\n",
      "--- Missing Values ---\n",
      "Channel             0\n",
      "Region              0\n",
      "Fresh               0\n",
      "Milk                0\n",
      "Grocery             0\n",
      "Frozen              0\n",
      "Detergents_Paper    0\n",
      "Delicassen          0\n",
      "\n",
      "--- Duplicates: 0 ---\n",
      "\n",
      "--- Statistics ---\n",
      "       Channel  Region      Fresh      Milk   Grocery    Frozen  Detergents_Paper  Delicassen\n",
      "count   440.00  440.00     440.00    440.00    440.00    440.00            440.00      440.00\n",
      "mean      1.32    2.54   12000.30   5796.27   7951.28   3071.93           2881.49     1524.87\n",
      "std       0.47    0.77   12647.33   7380.38   9503.16   4854.67           4767.85     2820.11\n",
      "min       1.00    1.00       3.00     55.00      3.00     25.00              3.00        3.00\n",
      "25%       1.00    2.00    3127.75   1533.00   2153.00    742.25            256.75      408.25\n",
      "50%       1.00    3.00    8504.00   3627.00   4755.50   1526.00            816.50      965.50\n",
      "75%       2.00    3.00   16933.75   7190.25  10655.75   3554.25           3922.00     1820.25\n",
      "max       2.00    3.00  112151.00  73498.00  92780.00  60869.00          40827.00    47943.00\n",
      "\n",
      "--- Channel Distribution ---\n",
      "Channel\n",
      "1    298\n",
      "2    142\n",
      "\n",
      "--- Region Distribution ---\n",
      "Region\n",
      "3    316\n",
      "1     77\n",
      "2     47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# 2.1 Basic info\n",
    "print(f\"\\n--- Data Types ---\")\n",
    "print(df.dtypes.to_string())\n",
    "\n",
    "print(f\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum().to_string())\n",
    "\n",
    "print(f\"\\n--- Duplicates: {df.duplicated().sum()} ---\")\n",
    "\n",
    "print(f\"\\n--- Statistics ---\")\n",
    "print(df.describe().round(2).to_string())\n",
    "\n",
    "# 2.2 Channel & Region distribution\n",
    "print(f\"\\n--- Channel Distribution ---\")\n",
    "print(df[\"Channel\"].value_counts().to_string())\n",
    "print(f\"\\n--- Region Distribution ---\")\n",
    "print(df[\"Region\"].value_counts().to_string())\n",
    "\n",
    "# 2.3 Basic plots\n",
    "spending_cols = [\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\",\n",
    "                 \"Detergents_Paper\", \"Delicassen\"]\n",
    "\n",
    "# Histograms\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(spending_cols):\n",
    "    axes[i].hist(df[col], bins=25, color=\"steelblue\", edgecolor=\"black\")\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(\"Spending\")\n",
    "    axes[i].set_ylabel(\"Count\")\n",
    "plt.suptitle(\"Spending Distribution (Before Cleaning)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"01_distributions.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Boxplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(spending_cols):\n",
    "    axes[i].boxplot(df[col], vert=True)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_ylabel(\"Spending\")\n",
    "plt.suptitle(\"Boxplots — Outlier Check\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"02_boxplots.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Correlation\n",
    "print(f\"\\n--- Correlation Matrix ---\")\n",
    "print(df[spending_cols].corr().round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bc021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 3: DATA CLEANING\n",
    "# ============================================================\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 3.1 Drop Channel & Region (we want unsupervised — no labels)\n",
    "# But keep them aside for later verification\n",
    "labels_channel = df_clean[\"Channel\"].copy()\n",
    "labels_region = df_clean[\"Region\"].copy()\n",
    "\n",
    "df_clean = df_clean.drop(columns=[\"Channel\", \"Region\"])\n",
    "print(\" 1. Dropped Channel & Region (keeping for verification)\")\n",
    "\n",
    "# 3.2 Check & remove duplicates\n",
    "before = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "labels_channel = labels_channel[:len(df_clean)]\n",
    "labels_region = labels_region[:len(df_clean)]\n",
    "after = len(df_clean)\n",
    "print(f\" 2. Removed {before - after} duplicates\")\n",
    "\n",
    "# 3.3 Handle missing values\n",
    "missing = df_clean.isnull().sum().sum()\n",
    "print(f\" 3. Missing values: {missing}\")\n",
    "\n",
    "# 3.4 Handle outliers using IQR capping\n",
    "print(f\"\\n--- Outlier Capping (IQR method) ---\")\n",
    "for col in df_clean.columns:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers_count = ((df_clean[col] < lower) | (df_clean[col] > upper)).sum()\n",
    "    df_clean[col] = df_clean[col].clip(lower=lower, upper=upper)\n",
    "    print(f\"   {col:>20s}: {outliers_count:>3d} outliers capped\")\n",
    "\n",
    "print(f\"\\n 4. All outliers capped using IQR\")\n",
    "\n",
    "# 3.5 Log transformation (spending data is skewed)\n",
    "df_log = df_clean.copy()\n",
    "for col in df_clean.columns:\n",
    "    df_log[col] = np.log1p(df_clean[col])\n",
    "print(\" 5. Log transformation applied (skewness fix)\")\n",
    "\n",
    "# 3.6 Feature scaling\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_log),\n",
    "    columns=df_log.columns\n",
    ")\n",
    "print(\" 6. StandardScaler applied\")\n",
    "\n",
    "print(f\"\\n--- Cleaned & Scaled Data (first 5 rows) ---\")\n",
    "print(df_scaled.head().round(3).to_string())\n",
    "print(f\"\\nFinal shape: {df_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 4: FIND OPTIMAL CLUSTERS\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# 4.1 Dendrogram\n",
    "print(\"\\n--- Generating Dendrogram ---\")\n",
    "linkage_matrix = linkage(df_scaled, method=\"ward\", metric=\"euclidean\")\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    truncate_mode=\"lastp\",\n",
    "    p=30,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=10,\n",
    "    show_contracted=True\n",
    ")\n",
    "plt.title(\"Dendrogram (Ward Linkage)\", fontweight=\"bold\", fontsize=14)\n",
    "plt.xlabel(\"Sample Index / Cluster Size\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.axhline(y=10, color=\"red\", linestyle=\"--\", label=\"Cut at distance=10\")\n",
    "plt.axhline(y=15, color=\"orange\", linestyle=\"--\", label=\"Cut at distance=15\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"03_dendrogram.png\", dpi=100)\n",
    "plt.show()\n",
    "print(\" Dendrogram saved\")\n",
    "\n",
    "# 4.2 Silhouette Score for different K values\n",
    "print(\"\\n--- Silhouette Scores for K=2 to K=10 ---\")\n",
    "sil_scores = {}\n",
    "\n",
    "for k in range(2, 11):\n",
    "    agg = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
    "    labels = agg.fit_predict(df_scaled)\n",
    "    sil = silhouette_score(df_scaled, labels)\n",
    "    ch = calinski_harabasz_score(df_scaled, labels)\n",
    "    db = davies_bouldin_score(df_scaled, labels)\n",
    "    sil_scores[k] = {\"Silhouette\": sil, \"Calinski\": ch, \"Davies_Bouldin\": db}\n",
    "    print(f\"   K={k:>2d}  |  Silhouette: {sil:.4f}  |  \"\n",
    "          f\"Calinski-Harabasz: {ch:.1f}  |  Davies-Bouldin: {db:.4f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "ks = list(sil_scores.keys())\n",
    "sils = [sil_scores[k][\"Silhouette\"] for k in ks]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ks, sils, \"bo-\", linewidth=2, markersize=8)\n",
    "plt.xlabel(\"Number of Clusters (K)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs K\", fontweight=\"bold\")\n",
    "plt.xticks(ks)\n",
    "for k, s in zip(ks, sils):\n",
    "    plt.annotate(f\"{s:.3f}\", (k, s), textcoords=\"offset points\",\n",
    "                 xytext=(0, 10), ha=\"center\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"04_silhouette_scores.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# Find best K\n",
    "best_k = max(sil_scores, key=lambda k: sil_scores[k][\"Silhouette\"])\n",
    "print(f\"\\n Best K = {best_k} (Silhouette = {sil_scores[best_k]['Silhouette']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 5: FIT WITH DIFFERENT LINKAGES & COMPARE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "linkages = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "linkage_results = []\n",
    "\n",
    "print(f\"\\n{'Linkage':<12} {'Silhouette':>12} {'Calinski-H':>12} \"\n",
    "      f\"{'Davies-B':>12} {'Status':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "best_model = None\n",
    "best_labels = None\n",
    "best_score = -1\n",
    "best_linkage = \"\"\n",
    "\n",
    "for link in linkages:\n",
    "    try:\n",
    "        if link == \"ward\":\n",
    "            agg = AgglomerativeClustering(\n",
    "                n_clusters=best_k, linkage=link\n",
    "            )\n",
    "        else:\n",
    "            agg = AgglomerativeClustering(\n",
    "                n_clusters=best_k, linkage=link\n",
    "            )\n",
    "\n",
    "        cluster_labels = agg.fit_predict(df_scaled)\n",
    "\n",
    "        sil = silhouette_score(df_scaled, cluster_labels)\n",
    "        ch = calinski_harabasz_score(df_scaled, cluster_labels)\n",
    "        db = davies_bouldin_score(df_scaled, cluster_labels)\n",
    "\n",
    "        linkage_results.append({\n",
    "            \"Linkage\": link,\n",
    "            \"Silhouette\": round(sil, 4),\n",
    "            \"Calinski_Harabasz\": round(ch, 2),\n",
    "            \"Davies_Bouldin\": round(db, 4),\n",
    "            \"Labels\": cluster_labels\n",
    "        })\n",
    "\n",
    "        status = \"\"\n",
    "        if sil > best_score:\n",
    "            best_score = sil\n",
    "            best_model = agg\n",
    "            best_labels = cluster_labels\n",
    "            best_linkage = link\n",
    "            status = \" BEST\"\n",
    "\n",
    "        print(f\"{link:<12} {sil:>12.4f} {ch:>12.1f} {db:>12.4f} {status:>10}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{link:<12} {'ERROR':>12} — {str(e)}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "print(f\"\\n Best Linkage: {best_linkage.upper()}\")\n",
    "print(f\"   Silhouette Score: {best_score:.4f}\")\n",
    "print(f\"   Number of Clusters: {best_k}\")\n",
    "\n",
    "# Compare linkages visually\n",
    "results_df = pd.DataFrame(linkage_results).drop(columns=[\"Labels\"])\n",
    "print(f\"\\n--- Linkage Comparison Table ---\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d98fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 6: ASSIGN CLUSTERS & ANALYZE\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "# Add cluster labels to original cleaned data\n",
    "df_result = df_clean.copy()\n",
    "df_result[\"Cluster\"] = best_labels\n",
    "\n",
    "# 6.1 Cluster sizes\n",
    "print(f\"\\n--- Cluster Sizes ---\")\n",
    "cluster_counts = df_result[\"Cluster\"].value_counts().sort_index()\n",
    "for cl, cnt in cluster_counts.items():\n",
    "    print(f\"   Cluster {cl}: {cnt} customers ({cnt/len(df_result)*100:.1f}%)\")\n",
    "\n",
    "# 6.2 Cluster means (spending profile)\n",
    "print(f\"\\n--- Cluster Spending Profiles (Mean) ---\")\n",
    "cluster_means = df_result.groupby(\"Cluster\").mean().round(1)\n",
    "print(cluster_means.to_string())\n",
    "\n",
    "# 6.3 Cluster means (median)\n",
    "print(f\"\\n--- Cluster Spending Profiles (Median) ---\")\n",
    "cluster_medians = df_result.groupby(\"Cluster\").median().round(1)\n",
    "print(cluster_medians.to_string())\n",
    "\n",
    "# 6.4 Describe each cluster\n",
    "print(f\"\\n--- Cluster Descriptions ---\")\n",
    "overall_mean = df_clean.mean()\n",
    "for cl in sorted(df_result[\"Cluster\"].unique()):\n",
    "    cluster_data = df_result[df_result[\"Cluster\"] == cl]\n",
    "    cl_mean = cluster_data[spending_cols].mean()\n",
    "    print(f\"\\n   CLUSTER {cl} ({len(cluster_data)} customers):\")\n",
    "\n",
    "    high_features = []\n",
    "    low_features = []\n",
    "    for col in spending_cols:\n",
    "        if cl_mean[col] > overall_mean[col] * 1.2:\n",
    "            high_features.append(col)\n",
    "        elif cl_mean[col] < overall_mean[col] * 0.8:\n",
    "            low_features.append(col)\n",
    "\n",
    "    if high_features:\n",
    "        print(f\"     HIGH spending: {', '.join(high_features)}\")\n",
    "    if low_features:\n",
    "        print(f\"     LOW  spending: {', '.join(low_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 7: VISUALIZATIONS (Simple & Basic)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# 7.1 PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\",\n",
    "          \"brown\", \"pink\", \"gray\", \"cyan\", \"magenta\"]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "for cl in sorted(df_result[\"Cluster\"].unique()):\n",
    "    mask = best_labels == cl\n",
    "    plt.scatter(df_pca[mask, 0], df_pca[mask, 1],\n",
    "                c=colors[cl], label=f\"Cluster {cl}\",\n",
    "                s=50, edgecolors=\"black\", linewidths=0.5, alpha=0.7)\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\")\n",
    "plt.title(f\"Agglomerative Clustering (K={best_k}, {best_linkage})\",\n",
    "          fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"05_clusters_pca.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# 7.2 Cluster size bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(cluster_counts.index, cluster_counts.values,\n",
    "               color=[colors[i] for i in cluster_counts.index],\n",
    "               edgecolor=\"black\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.title(\"Cluster Sizes\", fontweight=\"bold\")\n",
    "plt.xticks(cluster_counts.index)\n",
    "for bar, val in zip(bars, cluster_counts.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             str(val), ha=\"center\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"06_cluster_sizes.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# 7.3 Cluster spending profile (grouped bar chart)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(spending_cols):\n",
    "    cl_means = df_result.groupby(\"Cluster\")[col].mean()\n",
    "    axes[i].bar(cl_means.index, cl_means.values,\n",
    "                color=[colors[c] for c in cl_means.index],\n",
    "                edgecolor=\"black\")\n",
    "    axes[i].set_title(f\"{col}\", fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"Cluster\")\n",
    "    axes[i].set_ylabel(\"Mean Spending\")\n",
    "    axes[i].set_xticks(cl_means.index)\n",
    "\n",
    "plt.suptitle(\"Average Spending per Cluster\", fontweight=\"bold\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"07_spending_profiles.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# 7.4 Compare with actual Channel labels\n",
    "print(f\"\\n--- Cluster vs Actual Channel (Verification) ---\")\n",
    "comparison = pd.crosstab(\n",
    "    best_labels[:len(labels_channel)],\n",
    "    labels_channel[:len(best_labels)],\n",
    "    rownames=[\"Cluster\"],\n",
    "    colnames=[\"Channel\"]\n",
    ")\n",
    "print(comparison.to_string())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "comparison.plot(kind=\"bar\", edgecolor=\"black\")\n",
    "plt.title(\"Clusters vs Actual Channel\", fontweight=\"bold\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Channel\", labels=[\"Hotel/Restaurant\", \"Retail\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"08_cluster_vs_channel.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# 7.5 Linkage comparison bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "link_names = [r[\"Linkage\"] for r in linkage_results]\n",
    "link_sils = [r[\"Silhouette\"] for r in linkage_results]\n",
    "bar_colors = [\"green\" if l == best_linkage else \"gray\" for l in link_names]\n",
    "\n",
    "bars = plt.bar(link_names, link_sils, color=bar_colors, edgecolor=\"black\")\n",
    "plt.xlabel(\"Linkage Method\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Linkage Comparison\", fontweight=\"bold\")\n",
    "for bar, val in zip(bars, link_sils):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f\"{val:.4f}\", ha=\"center\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"09_linkage_comparison.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(\" All plots saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 8: PREDICT NEW DATA\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def predict_new_customer(fresh, milk, grocery, frozen,\n",
    "                         detergents_paper, delicassen):\n",
    "    \"\"\"Predict cluster for a new customer.\"\"\"\n",
    "\n",
    "    new_data = pd.DataFrame([{\n",
    "        \"Fresh\": fresh,\n",
    "        \"Milk\": milk,\n",
    "        \"Grocery\": grocery,\n",
    "        \"Frozen\": frozen,\n",
    "        \"Detergents_Paper\": detergents_paper,\n",
    "        \"Delicassen\": delicassen\n",
    "    }])\n",
    "\n",
    "    # Apply same transformations\n",
    "    # 1. IQR capping (using training stats)\n",
    "    for col in new_data.columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        new_data[col] = new_data[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "    # 2. Log transform\n",
    "    new_data_log = new_data.apply(np.log1p)\n",
    "\n",
    "    # 3. Scale\n",
    "    new_data_scaled = pd.DataFrame(\n",
    "        scaler.transform(new_data_log),\n",
    "        columns=new_data_log.columns\n",
    "    )\n",
    "\n",
    "    # 4. Find nearest cluster using centroid distance\n",
    "    centroids = df_scaled.copy()\n",
    "    centroids[\"Cluster\"] = best_labels\n",
    "    cluster_centers = centroids.groupby(\"Cluster\").mean()\n",
    "\n",
    "    distances = {}\n",
    "    for cl in cluster_centers.index:\n",
    "        center = cluster_centers.loc[cl].values\n",
    "        point = new_data_scaled.values[0]\n",
    "        dist = np.sqrt(np.sum((point - center) ** 2))\n",
    "        distances[cl] = round(dist, 4)\n",
    "\n",
    "    predicted_cluster = min(distances, key=distances.get)\n",
    "\n",
    "    return predicted_cluster, distances\n",
    "\n",
    "\n",
    "# Example predictions\n",
    "print(\"\\n--- Example Prediction 1: High Fresh Spender ---\")\n",
    "cluster, dists = predict_new_customer(\n",
    "    fresh=20000, milk=3000, grocery=4000,\n",
    "    frozen=2000, detergents_paper=500, delicassen=1000\n",
    ")\n",
    "print(f\"   Input: Fresh=20000, Milk=3000, Grocery=4000, \"\n",
    "      f\"Frozen=2000, Detergents=500, Delicassen=1000\")\n",
    "print(f\"   Distances: {dists}\")\n",
    "print(f\"    Predicted Cluster: {cluster}\")\n",
    "\n",
    "print(\"\\n--- Example Prediction 2: High Grocery Spender ---\")\n",
    "cluster, dists = predict_new_customer(\n",
    "    fresh=5000, milk=15000, grocery=25000,\n",
    "    frozen=1000, detergents_paper=12000, delicassen=2000\n",
    ")\n",
    "print(f\"   Input: Fresh=5000, Milk=15000, Grocery=25000, \"\n",
    "      f\"Frozen=1000, Detergents=12000, Delicassen=2000\")\n",
    "print(f\"   Distances: {dists}\")\n",
    "print(f\"    Predicted Cluster: {cluster}\")\n",
    "\n",
    "print(\"\\n--- Example Prediction 3: Low Spender ---\")\n",
    "cluster, dists = predict_new_customer(\n",
    "    fresh=3000, milk=2000, grocery=2000,\n",
    "    frozen=500, detergents_paper=300, delicassen=400\n",
    ")\n",
    "print(f\"   Input: Fresh=3000, Milk=2000, Grocery=2000, \"\n",
    "      f\"Frozen=500, Detergents=300, Delicassen=400\")\n",
    "print(f\"   Distances: {dists}\")\n",
    "print(f\"    Predicted Cluster: {cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68903792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 9: COMPLETE EVALUATION SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "final_sil = silhouette_score(df_scaled, best_labels)\n",
    "final_ch = calinski_harabasz_score(df_scaled, best_labels)\n",
    "final_db = davies_bouldin_score(df_scaled, best_labels)\n",
    "\n",
    "\n",
    "\n",
    "for cl in sorted(df_result[\"Cluster\"].unique()):\n",
    "    cnt = len(df_result[df_result[\"Cluster\"] == cl])\n",
    "    pct = cnt / len(df_result) * 100\n",
    "    print(f\"  Cluster {cl}: {cnt:>4d} customers ({pct:>5.1f}%) \")\n",
    "\n",
    "\n",
    "\n",
    "for r in linkage_results:\n",
    "    marker = \" \" if r[\"Linkage\"] == best_linkage else \"   \"\n",
    "    print(f\"  {r['Linkage']:<10s}: Silhouette={r['Silhouette']:.4f}{marker} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abf166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# STEP 10: SAVE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" STEP 10: SAVE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save clustered data\n",
    "df_result.to_csv(\"clustered_customers.csv\", index=False)\n",
    "print(\" Saved: clustered_customers.csv\")\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_means.to_csv(\"cluster_profiles.csv\")\n",
    "print(\" Saved: cluster_profiles.csv\")\n",
    "\n",
    "# Save evaluation\n",
    "eval_df = pd.DataFrame(linkage_results).drop(columns=[\"Labels\"])\n",
    "eval_df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"   01_distributions.png\")\n",
    "print(\"   02_boxplots.png\")\n",
    "print(\"   03_dendrogram.png\")\n",
    "print(\"   04_silhouette_scores.png\")\n",
    "print(\"   05_clusters_pca.png\")\n",
    "print(\"   06_cluster_sizes.png\")\n",
    "print(\"   07_spending_profiles.png\")\n",
    "print(\"   08_cluster_vs_channel.png\")\n",
    "print(\"   09_linkage_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
